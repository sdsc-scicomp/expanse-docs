{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Expanse Supercomputer User Guide","text":"<p>Welcome to the user guide for the Expanse Supercomputer at SDSC. Use the navigation on the left to explore the documentation.</p>"},{"location":"account_management/","title":"Account Management","text":""},{"location":"account_management/#managing-your-user-account","title":"Managing Your User Account","text":"<p>The <code>expanse-client</code> script provides additional details regarding project availability and usage. The script is located at:</p> <pre><code>/cm/shared/apps/sdsc/current/bin/expanse-client\n</code></pre> <p>The script uses the 'sdsc' module, which is loaded by default.</p> <pre><code>[user@login01 ~]$ module load sdsc\n</code></pre> <p>To review your available projects on Expanse resource use the 'user' parameter and '-r' to desginate a resource. If no resouce is designated expanse data will be shown by default.</p> <pre><code>user@login01 ~]$ expanse-client user -r expanse\n</code></pre> <pre><code> Resource expanse\n\u256d\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502   \u2502 NAME        \u2502 PROJECT \u2502 TG PROJECT \u2502 USED \u2502 AVAILABLE \u2502 USED BY PROJECT \u2502\n\u251c\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 1 \u2502 user        \u2502 ddp386  \u2502            \u2502 0    \u2502 110000    \u2502 8318            \u2502\n\u2570\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre> <p>To see full list of available resources, use the 'resource' command:</p> <pre><code>[user@login02 ~]$ expanse-client resource\n</code></pre> <pre><code>Available resources:\nexpanse\nexpanse_gpu\nexpanse_industry\nexpanse_industry_gpu\n</code></pre> <p>To review project details, use the 'project' parameter followed by an eligible project. (use -p option to report with out formatting):</p> <pre><code>user@login01 ~]$ expanse-client project ddp386 -p\nResource expanse  \nProject ddp386  \nTG Project  \nTotal allocation 110000  \nTotal spent 8318  \nExpiration November 16, 2022  \n\nNAME    USED    AVAILABLE    USED BY PROJECT  \n-------------------------------------------------  \nuser    0       110000       8318  \nuser1   0       110000       8318  \nuser2   18      110000       8318  \nuser3   7825    110000       8318  \nuser4   0       110000       8318  \nuser5   152     110000       8318  \n</code></pre> <p>For additional help using the expanse-client tool:</p> <pre><code>[user@login01 ~]$ expanse-client -h\nUsage:\n expanse-client [command][flags]\nAvailable Commands:\n help Help about any command\n project Get 'project' information\n resource Get resources\n user Get 'user' information\nFlags:\n -a, --auth authenticate the request \n -h, --help help for user\n -p, --plain plain no graphics output\n -v, --verbose verbose output\n -r, --resource string Resource to query (default: \"expanse\")\nGlobal Flags:\n -a, --auth authenticate the request\n -p, --plain plain no graphics output\n -v, --verbose verbose output\n</code></pre> <p>Many users will have access to multiple projects (e.g. an allocation for a research project and a separate allocation for classroom or educational use). Users should verify that the correct project is designated for all batch jobs. Awards are granted for a specific purposes and should not be used for other projects. Designate a project by replacing <code>&lt;&lt; project &gt;&gt;</code> with a project listed in the SBATCH directive in your job script:</p> <pre><code>  #SBATCH -A &lt;&lt; project &gt;&gt;\n</code></pre>"},{"location":"account_management/#adding-users-to-a-project","title":"Adding Users to a Project","text":"<p>Project PIs and co-PIs can add/remove users(accounts) to/from a project. To do this, log in to your ACCESS portal account and go to the Manage Allocations page.</p>"},{"location":"citations_and_publications/","title":"Citations &amp; Publications","text":""},{"location":"citations_and_publications/#how-to-cite-expanse","title":"How to cite Expanse","text":"<p>We request that you cite your use of the Expanse with the following citation format, and modified as needed to conform with citation style guidelines. Most importantly, please include the Digital Object Identifier (DOI) --- https://doi.org/10.1145/3437359.3465588 --- that is unique to Expanse.</p>"},{"location":"citations_and_publications/#example","title":"Example","text":"<p><code>San Diego Supercomputer Center (2025): Expanse. University of California San Diego. Service. https://doi.org/10.1145/3437359.3465588</code></p>"},{"location":"citations_and_publications/#publications","title":"Publications","text":"<p>View a list of publications resulting from the use of Expanse.</p>"},{"location":"compiling_codes/","title":"Compiling Codes","text":"<p>Expanse CPU nodes have GNU, Intel, and AOCC (AMD) compilers available along with multiple MPI implementations (OpenMPI, MVAPICH2, and IntelMPI). The majority of the applications on Expanse have been built using gcc/10.2.0 which features AMD Rome specific optimization flags (-march=znver2). Users should evaluate their application for best compiler and library selection. GNU, Intel, and AOCC compilers all have flags to support Advanced Vector Extensions 2 (AVX2). Using AVX2, up to eight floating point operations can be executed per cycle per core, potentially doubling the performance relative to non-AVX2 processors running at the same clock speed. Note that AVX2 support is not enabled by default and compiler flags must be set as described below.</p> <p>Expanse GPU nodes have GNU, Intel, and PGI compilers available along with multiple MPI implementations (OpenMPI, IntelMPI, and MVAPICH2). The gcc/10.2.0, Intel, and PGI compilers have specific flags for the Cascade Lake architecture. Users should evaluate their application for best compiler and library selections.</p> <p>Note that the login nodes are not the same as the GPU nodes, therefore all GPU codes must be compiled by requesting an interactive session on the GPU nodes.</p>"},{"location":"compiling_codes/#using-amd-compilers","title":"Using AMD Compilers","text":"<p>The AMD Optimizing C/C++ Compiler (AOCC) is only available on CPU nodes. AMD compilers can be loaded by executing the following commands at the Linux prompt:</p> <pre><code>module load aocc\n</code></pre> <p>For more information on the AMD compilers: <code>[flang | clang ] -help</code></p> Serial MPI OpenMP MPI+OpenMP Fortran <code>flang</code> <code>mpif90</code> <code>ifort -mp</code> <code>mpif90 -fopenmp</code> C <code>clang</code> <code>mpiclang</code> <code>icc -lomp</code> <code>mpicc -fopenmp</code> C++ <code>clang++</code> <code>mpiclang</code> <code>icpc -lomp</code> <code>mpicxx -fopenmp</code>"},{"location":"compiling_codes/#using-the-intel-compilers","title":"Using the Intel Compilers","text":"<p>The Intel compilers and the MVAPICH2 MPI compiler wrappers can be loaded by executing the following commands at the Linux prompt:</p> <pre><code>module load intel mvapich2\n</code></pre> <p>For AVX2 support, compile with the <code>-march=core-avx2</code> option. Note that this flag alone does not enable aggressive optimization, so compilation with <code>-O3</code> is also suggested. Intel MKL libraries are available as part of the \"intel\" modules on Expanse. Once this module is loaded, the environment variable INTEL_MKLHOME points to the location of the mkl libraries. The MKL link advisor can be used to ascertain the link line (change the INTEL_MKLHOME aspect appropriately). For example to compile a C program statically linking 64 bit scalapack libraries on Expanse :</p> <pre><code>mpicc -o pdpttr.exe pdpttr.c \\\n    -I$INTEL_MKLHOME/mkl/include \\\n    ${INTEL_MKLHOME}/mkl/lib/intel64/libmkl_scalapack_lp64.a \\\n    -Wl,--start-group ${INTEL_MKLHOME}/mkl/lib/intel64/libmkl_intel_lp64.a \\\n    ${INTEL_MKLHOME}/mkl/lib/intel64/libmkl_core.a \\\n    ${INTEL_MKLHOME}/mkl/lib/intel64/libmkl_sequential.a \\\n    -Wl,--end-group ${INTEL_MKLHOME}/mkl/lib/intel64/libmkl_blacs_intelmpi_lp64.a \\\n    -lpthread -lm\n</code></pre> <p>For more information on the Intel compilers: <code>[ifort | icc | icpc] -help</code></p> Serial MPI OpenMP MPI+OpenMP Fortran <code>ifort</code> <code>mpif90</code> <code>ifort -qopenmp</code> <code>mpif90 -qopenmp</code> C <code>icc</code> <code>mpicc</code> <code>icc -qopenmp</code> <code>mpicc -qopenmp</code> C++ <code>icpc</code> <code>mpicxx</code> <code>icpc -qopenmp</code> <code>mpicxx -qopenmp</code>"},{"location":"compiling_codes/#using-the-pgi-compilers","title":"Using the PGI Compilers","text":"<p>The PGI compilers are only available on the GPU nodes, and can be loaded by executing the following commands at the Linux prompt</p> <pre><code>module load pgi\n</code></pre> <p>Note that the openmpi build is integrated into the PGI install so the above module load provides both PGI and openmpi. For AVX support, compile with <code>-fast</code>. For more information on the PGI compilers: <code>man [pgf90 | pgcc | pgCC]</code></p> Serial MPI OpenMP MPI+OpenMP Fortran <code>pgf90</code> <code>mpif90</code> <code>pgf90 -mp</code> <code>mpif90 -mp</code> C <code>pgcc</code> <code>mpicc</code> <code>pgcc -mp</code> <code>mpicc -mp</code> C++ <code>pgCC</code> <code>mpicxx</code> <code>pgCC -mp</code> <code>mpicxx -mp</code>"},{"location":"compiling_codes/#using-the-gnu-compilers","title":"Using the GNU Compilers","text":"<p>The GNU compilers can be loaded by executing the following commands at the Linux prompt:</p> <pre><code>module load gcc openmpi\n</code></pre> <p>For AVX support, compile with -march=core-avx2. Note that AVX support is only available in version 4.7 or later, so it is necessary to explicitly load the gnu/4.9.2 module until such time that it becomes the default. For more information on the GNU compilers: <code>man [gfortran | gcc | g++]</code></p> Serial MPI OpenMP MPI+OpenMP Fortran <code>gfortran</code> <code>mpif90</code> <code>gfortran -fopenmp</code> <code>mpif90 -fopenmp</code> C <code>gcc</code> <code>mpicc</code> <code>gcc -fopenmp</code> <code>mpicc -fopenmp</code> C++ <code>g++</code> <code>mpicxx</code> <code>g++ -fopenmp</code> <code>mpicxx -fopenmp</code>"},{"location":"compiling_codes/#notes-and-hints","title":"Notes and Hints","text":"<ul> <li>The mpif90, mpicc, and mpicxx commands are actually wrappers that call the appropriate serial compilers and load the correct MPI libraries. While the same names are used for the Intel, PGI and GNU compilers, keep in mind that these are completely independent scripts.</li> <li>If you use the PGI or GNU compilers or switch between compilers for different applications, make sure that you load the appropriate modules before running your executables.</li> <li>When building OpenMP applications and moving between different compilers, one of the most common errors is to use the wrong flag to enable handling of OpenMP directives. Note that Intel, PGI, and GNU compilers use the -qopenmp, -mp, and -fopenmp flags, respectively.</li> <li>Explicitly set the optimization level in your makefiles or compilation scripts. Most well written codes can safely use the highest optimization level (-O3), but many compilers set lower default levels (e.g. GNU compilers use the default -O0, which turns off all optimizations).</li> <li>Turn off debugging, profiling, and bounds checking when building executables intended for production runs as these can seriously impact performance. These options are all disabled by default. The flag used for bounds checking is compiler dependent, but the debugging (-g) and profiling (-pg) flags tend to be the same for all major compilers.</li> </ul>"},{"location":"composable_systems/","title":"Composable Systems","text":"<p>Expanse also supports Composable Systems, allowing researchers to create a virtual 'tool set' of resources, such as Kubernetes resources, for a specific project and then re-compose it as needed. Expanse will also feature direct scheduler integration with the major cloud providers, leveraging high-speed networks to ease data movement to and from the cloud.</p> <p>All Composable System requests must include a brief justification, specifically describing why a Composable System is required for the project.</p>"},{"location":"data_movement/","title":"Data Movement","text":""},{"location":"data_movement/#globus-sdsc-collections-data-movers-and-mount-points","title":"Globus: SDSC Collections, Data Movers and Mount Points","text":"<p>All of Expanse's Lustre filesystems are accessible via the SDSC Expanse specific collections (SDSC HPC - Expanse Lustre; SDSC HPC - Projects). The following table shows the mount points on the data mover nodes that are the backend for Globus.</p> Machine Location on machine Location on Globus/Data Movers Expanse <code>/expanse/projects</code> / Expanse <code>/expanse/lustre/projects</code> <code>/projects/...</code> Expanse <code>/expanse/lustre/scratch</code> <code>/scratch/...</code>"},{"location":"job_charging/","title":"Job Charging","text":"<p>The charge unit for all SDSC machines, including Expanse, is the Service Unit (SU). This corresponds to the equivalent use of one compute core utilizing less than or equal to 2G of data for one hour, or 1 GPU using less than 92G of data for 1 hour. Keep in mind that your charges are based on the resources that are tied up by your job and don't necessarily reflect how the resources are used. Charges on jobs submitted to the 'shared' partitions (shared,gpu-shared,debug,gpu-debug,large-shared) are based on either the number of cores or the fraction of the memory requested, whichever is larger. Jobs submitted to the node-exclusive partitions (compute, gpu) will be charged for the all 128 cores, whether the resources are used or not. The minimum charge for any job is 1 SU.</p>"},{"location":"job_charging/#job-charge-considerations","title":"Job Charge Considerations","text":"<ul> <li>A node-exclusive job that runs on a compute node for one hour will be charged 128 SUs (128 cores x 1 hour)</li> <li>A node-exclusive job that runs on a GPU node for one hour will be charge 4GPU hours (4 GPU x 1 hour)</li> <li>A node-exclusive job that runs on a Large memory node for one hour will be charged 1024 SUs (2TB memory X 1 hour)</li> <li>A serial job in the shared queue that uses less than 2 GB memory and runs for one hour will be charged 1 SU (1 core x 1 hour)</li> <li>Each standard compute node has ~256 GB of memory and 128 cores<ul> <li>Each standard node core will be allocated 1 GB of memory, users should explicitly include the <code>--mem</code> directive to request additional memory</li> <li>Max. available memory per compute node <code>--mem = 249208M</code></li> </ul> </li> <li>Each GPU node has 4 GPUs, ~384GB of memory and 40 cores<ul> <li>Default resource allocation for 1 GPU = 1 GPU, 1 CPU, and 1G of memory, users will need to explicitly ask for additional resources in their job script.</li> <li>For max memory on a GPU node, users should request <code>--mem = 377308M</code></li> <li>A GPU SU is equivalent to 1GPU, &lt;10CPUs, and &lt;92G of memory.</li> </ul> </li> <li>Multicore jobs will scale according to resource utilization</li> <li>Each large memory node has ~2 TB of memory and 128 cores<ul> <li>By default the system will only allocate 1 GB of memory per core, explicitly use the <code>--mem</code> directive to request additional memory</li> <li>Max. memory per large memory node <code>--mem = 2055638M</code></li> </ul> </li> </ul>"},{"location":"modules/","title":"Modules","text":"<p>Environment Modules provide for dynamic modification of your shell environment. Module commands set, change, or delete environment variables, typically in support of a particular application. They also let the user choose between different versions of the same software or different combinations of related codes. Expanse uses Lmod, a Lua-based module system. Users will now need to setup their own environment by loading available modules into the shell environment, including compilers and libraries and the batch scheduler. Users will not see all the available modules when they run the <code>module available</code> command without loading a compiler. Users should use the command <code>module spider</code> to see if a particular package exists and can be loaded on the system. For additional details, and to identify dependents modules, use the command:</p> <pre><code>module spider &lt;application_name&gt;\n</code></pre> <p>The module paths are different for the CPU and GPU nodes. Users can enable the paths by loading the following modules:</p> <pre><code>module load cpu # for cpu nodes\n</code></pre> <pre><code>module load gpu # for gpu nodes\n</code></pre> <p>Users are requested to ensure that both sets are not loaded at the same time in their build/run environment (use the <code>module list</code> command to check in an interactive session). On the GPU nodes, the gnu compiler used for building packages is the default version 8.3.1 from the OS. Hence, no additional <code>module load</code> command is required to use them. For example, if one needs OpenMPI built with gnu compilers, the following is sufficient:</p> <pre><code>module load openmpi\n</code></pre>"},{"location":"modules/#useful-modules-commands","title":"Useful Modules Commands","text":"<p>Here are some common module commands and their descriptions:</p> Command Description <code>module list</code> List the modules that are currently loaded <code>module avail</code> List the modules that are available in environment <code>module spider</code> List of the modules and extensions currently available <code>module display &lt;module_name&gt;</code> Show the environment variables used by  and how they are affected <code>module unload &lt;module name&gt;</code> Remove  from the environment <code>module load &lt;module name&gt;</code> Load  into the environment <code>module swap &lt;module one&gt; &lt;module two&gt;</code> Replace  with  in the environment"},{"location":"modules/#loading-and-unloading-modules","title":"Loading and unloading modules","text":"<p>Some modules depend on others, so they may be loaded or unloaded as a consequence of another module command. If a model has dependencies, the command <code>module spider &lt;module_name&gt;</code> will provide additional details.</p> <p>Module: command not found</p> <p>The error message module: command not found is sometimes encountered when switching from one shell to another or attempting to run the module command from within a shell script or batch job. The reason the <code>module</code> command may not be inherited as expected is that it is defined as a function for your login shell. If you encounter this error, execute the following from the command line (interactive shells) or add to your shell script (including SLURM batch scripts):</p> <pre><code>source /etc/profile.d/modules.sh\n</code></pre>"},{"location":"running_jobs_on_expanse/","title":"Running Jobs on Expanse","text":"<p>Expanse uses the Simple Linux Utility for Resource Management (SLURM) batch environment. When you run in the batch mode, you submit jobs to be run on the compute nodes using the sbatch command as described below. Remember that computationally intensive jobs should be run only on the compute nodes and not the login nodes.</p> <p>Expanse places limits on the number of jobs queued and running on a per group (allocation) and partition basis. Please note that submitting a large number of jobs (especially very short ones) can impact the overall scheduler response for all users. If you are anticipating submitting a lot of jobs, please contact the SDSC consulting staff before you submit them. We can work to check if there are bundling options that make your workflow more efficient and reduce the impact on the scheduler.</p> <p>The limits for each partition are noted in the table below. Partition limits are subject to change based on Early User Period evaluation.</p> Partition Name Max Walltime Max Nodes/Job Max Running Jobs Max Running + Queued Jobs Charge Factor Notes compute 48 hrs 32 32 64 1 Exclusive access to regular compute nodes; limit applies per group ind-compute 48 hrs 32 16 32 1 Exclusive access to Industry compute nodes; limit applies per group shared 48 hrs 1 4096 4096 1 Single-node jobs using fewer than 128 cores ind-shared 48 hrs 1 2048 2048 1 Single-node Industry jobs using fewer than 128 cores gpu 48 hrs 4 4 8 (32 Tres GPU) 1 Used for exclusive access to the GPU nodes ind-gpu 48 hrs 4 4 4 (8 Tres GPU) 1 Exclusive access to the Industry GPU nodes gpu-shared 48 hrs 1 24 24 (24 Tres GPU) 1 Single-node job using fewer than 4 GPUs ind-gpu-shared 48 hrs 1 24 24 (24 Tres GPU) 1 Single-node job using fewer than 4 Industry GPUs large-shared 48 hrs 1 1 4 1 Single-node jobs using large memory up to 2 TB (minimum memory required 256G) debug 30 min 2 1 2 1 Priority access to shared nodes set aside for testing of jobs with short walltime and limited resources gpu-debug 30 min 2 1 2 1 Priority access to gpu-shared nodes set aside for testing of jobs with short walltime and limited resources; max two gpus per job preempt 7 days 32 128 .8 Non-refundable discounted jobs to run on free nodes that can be pre-empted by jobs submitted to any other queue gpu-preempt 7 days 1 24 (24 Tres GPU) .8 Non-refundable discounted jobs to run on unallocated nodes that can be pre-empted by higher priority queues"},{"location":"running_jobs_on_expanse/#requesting-interactive-resources-using-srun","title":"Requesting interactive resources using srun","text":"<p>You can request an interactive session using the srun command. The following example will request one regular compute node, 4 cores, in the debug partition for 30 minutes.</p> <p>srun --partition=debug  --pty --account=&lt;&gt; --nodes=1 --ntasks-per-node=4 \\         --mem=8G -t 00:30:00 --wait=0 --export=ALL /bin/bash <p>The following example will request a GPU node, 10 cores, 1 GPU and 96G in the debug partition for 30 minutes. To ensure the GPU environment is properly loaded, please be sure run both the <code>module purge</code> and <code>module restore</code> commands.</p> <pre><code>login01$ srun --partition=gpu-debug --pty --account=&lt;&lt;project&gt;&gt; --ntasks-per-node=10 \\\n    --nodes=1 --mem=96G --gpus=1 -t 00:30:00 --wait=0 --export=ALL /bin/bash\n\nsrun: job 1336890 queued and waiting for resources\n\nsrun: job 1336890 has been allocated resources\nexp-7-59$ module purge\nexp-7-59$ module restore\n\nResetting modules to system default. Resetting $MODULEPATH back to system default.\n    All extra directories will be removed from $MODULEPATH.\n</code></pre>"},{"location":"running_jobs_on_expanse/#submitting-jobs-using-sbatch","title":"Submitting Jobs Using sbatch","text":"<p>Jobs can be submitted to the sbatch partitions using the sbatch command as follows:</p> <pre><code>sbatch jobscriptfile\n</code></pre> <p>where <code>jobscriptfile</code> is the name of a UNIX format file containing special statements (corresponding to sbatch options), resource specifications and shell commands. Several example SLURM scripts are given below:</p>"},{"location":"running_jobs_on_expanse/#basic-mpi-job","title":"Basic MPI Job","text":"<pre><code>#!/bin/bash\n#SBATCH --job-name=\"hellompi\"\n#SBATCH --output=\"hellompi.%j.%N.out\"\n#SBATCH --partition=compute\n#SBATCH --nodes=2\n#SBATCH --ntasks-per-node=128\n#SBATCH --mem=0\n#SBATCH --account=&lt;&lt;project*&gt;&gt;\n#SBATCH --export=ALL\n#SBATCH -t 01:30:00\n\n#This job runs with 2 nodes, 128 cores per node for a total of 256 tasks.\n\nmodule purge\nmodule load cpu\n#Load module file(s) into the shell environment\nmodule load gcc\nmodule load mvapich2\nmodule load slurm\n\nsrun --mpi=pmi2 -n 256 ../hello_mpi\n</code></pre> <ul> <li>Expanse requires users to enter a valid project name; users can list valid project by running the<code>expanse-client</code> script.</li> <li>Expanse requires users to include memory , by using --mem=0 for compute partition the job will be allocated all the avialble memory on the node</li> </ul>"},{"location":"running_jobs_on_expanse/#basic-openmp-job","title":"Basic OpenMP Job","text":"<pre><code>#!/bin/bash\n#SBATCH --job-name=\"hello_openmp\"\n#SBATCH --output=\"hello_openmp.%j.%N.out\"\n#SBATCH --partition=compute\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=1\n#SBATCH --cpus-per-task=24\n#SBATCH --mem=249000M\n#SBATCH --account=&lt;&lt;project*&gt;&gt;\n#SBATCH --export=ALL\n#SBATCH -t 01:30:00\n\nmodule purge\nmodule load cpu\nmodule load slurm\nmodule load gcc\nmodule load openmpi\n\n#SET the number of openmp threads\nexport OMP_NUM_THREADS=24\n\n#Run the job\n./hello_openmp\n</code></pre> <p>* Expanse requires users to enter a valid project name; users can list valid project by running the<code>expanse-client</code> script.</p>"},{"location":"running_jobs_on_expanse/#hybrid-mpi-openmp-job","title":"Hybrid MPI-OpenMP Job","text":"<pre><code>#!/bin/bash\n#SBATCH --job-name=\"hellohybrid\"\n#SBATCH --output=\"hellohybrid.%j.%N.out\"\n#SBATCH --partition=compute\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=128\n#SBATCH --cpus-per-task=16\n#SBATCH --mem=249000M\n#SBATCH --account=&lt;&lt;project*&gt;&gt;\n#SBATCH --export=ALL\n#SBATCH -t 01:30:00\n\n#This job runs with 1 node, 128 cores per node for a total of 128 cores.\n# We use 8 MPI tasks and 16 OpenMP threads per MPI task\n\nmodule purge\nmodule load cpu\nmodule load slurm\nmodule load intel\nmodule load intel-mpi\n\nexport OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK\nmpirun -genv I_MPI_PIN_DOMAIN=omp:compact ./hello_hybrid\n</code></pre> <p>* Expanse require users to enter a valid project name; users can list valid project by running the<code>expanse-client</code> script.</p>"},{"location":"running_jobs_on_expanse/#using-the-shared-partition","title":"Using the Shared Partition","text":"<pre><code>#!/bin/bash\n#SBATCH -p shared\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=8\n#SBATCH --mem=40G\n#SBATCH -t 01:00:00\n#SBATCH -J job.8\n#SBATCH -A &lt;&lt;project*&gt;&gt;\n#SBATCH -o job.8.%j.%N.out\n#SBATCH -e job.8.%j.%N.err\n#SBATCH --export=ALL\n\nexport SLURM_EXPORT_ENV=ALL\n\nmodule purge\nmodule load cpu\nmodule load gcc\nmodule load mvapich2\nmodule load slurm\n\nsrun -n 8 ../hello_mpi\n</code></pre> <p>* Expanse requires users to enter a valid project name; users can list valid project by running the<code>expanse-client</code> script. The above script will run using 8 cores and 40 GB of memory. Please note that the performance in the shared partition may vary depending on how sensitive your application is to memory locality and the cores you are assigned by the scheduler. It is possible the 8 cores will span two sockets for example.</p>"},{"location":"running_jobs_on_expanse/#using-large-memory-nodes","title":"Using Large Memory Nodes","text":"<p>The large memory nodes can be accessed via the \"large-shared\" partition. Charges are based on either the number of cores or the fraction of the memory requested, whichever is larger. By default the system will only allocate 1 GB of memory per core. If additional memory is required, users should explicitly use the <code>--mem</code> directive. For example, on the \"large-shared\" partition, the following job requesting 128 cores and 2000 GB of memory (about 100% of 2TB of one node's available memory) for 1 hour will be charged 1024 SUs: 200/1455(memory) * 64(cores) * 1(duration) ~= 1024</p> <pre><code>#SBATCH --partition=large-shared\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=128\n#SBATCH --cpus-per-task=1\n#SBATCH --mem=2055638M\n\nexport OMP_PROC_BIND='true'\n</code></pre> <p>While there is not a separate 'large' partition, a job can still explicitly request all of the resources on a large memory node. Please note that there is no premium for using Expanse 's large memory nodes. Users are advised to request the large nodes only if they need the extra memory.</p>"},{"location":"software/","title":"Software","text":"<p>Expanse supports a broad application base with installs and modules for commonly used packages in bioinformatics, molecular dynamics, machine learning, quantum chemistry, structural mechanics, and visualization. Expanse will continue to support Singularity-based containerization. </p> <p>Users can search for available software on Expanse using the <code>module</code> command. For more details, see the Expanse Modules documentation.</p>"},{"location":"storage/","title":"Storage","text":""},{"location":"storage/#overview","title":"Overview","text":"<p>Users are responsible for backing up all important data in case of unexpected data loss at SDSC.</p> <p>The SDSC Expanse Lustre file system (including <code>/expanse/lustre/scratch</code> and <code>/expanse/lustre/project</code>) IS NOT an archival file system. The SDSC Expanse Lustre file system IS NOT backed up. SDSC will enforce a strict purge policy on the Expanse Lustre filesystem. Project space will be purged 90 days after allocation expires. Scratch files will be purged 90 days from creation date.</p>"},{"location":"storage/#local-scratch-disk","title":"Local Scratch Disk","text":"<p>The compute nodes on Expanse have access to fast flash storage. There is 1TB of SSD space available for use on each compute node. The latency to the SSDs is several orders of magnitude lower than that for spinning disk (&lt;100 microseconds vs. milliseconds) making them ideal for user-level check pointing and applications that need fast random I/O to large scratch files. Users can access the SSDs only during job execution under the following directories local to each compute node:</p> <pre><code>/scratch/$USER/job_$SLURM_JOB_ID\n</code></pre> Partition Space Available compute,shared 1 TB gpu, gpu-shared 1.6TB large-shared 3.2 TB"},{"location":"storage/#parallel-lustre-filesystems","title":"Parallel Lustre Filesystems","text":"<p>In addition to the local scratch storage, users will have access to global parallel filesystems on Expanse. Every Expanse node has access to a 12 PB Lustre parallel file system (provided by Aeon Computing) and a 7 PB Ceph Object Store system, 140 GB/second performance storage. SDSC limits the number of files that can be stored in the <code>/lustre/scratch</code> filesystem to 2 million files per user. Users should contact support for assistance at the ACCESS Help Desk if their workflow requires extensive small I/O, to avoid causing system issues assosiated with load on the metadata server.</p> <p>The two Lustre filesystems available on Expanse are:</p> <ul> <li>Lustre Expanse scratch filesystem: <code>/expanse/lustre/scratch/$USER/temp_project</code></li> <li>Lustre NSF projects filesystem: <code>/expanse/lustre/projects/</code></li> </ul>"},{"location":"storage/#submitting-jobs-using-lustre","title":"Submitting Jobs Using Lustre","text":"<p>Jobs that need to use the Lustre filesystem should explicitly reqeust the feature by including the following line to their script:</p> <pre><code>#SBATCH --constraint=\"lustre\"\n</code></pre> <p>This constraint can be used in combination with any other constraints you are already using. For example:</p> <pre><code>#SBATCH --constraint=\"lustre&amp;persistenceoff&amp;exclusive\"\n</code></pre> <p>Jobs submitted without --constraint=\"lustre\" that need the Lustre filesystem will be scheduled on nodes without Lustre and will FAIL.</p>"},{"location":"storage/#home-file-system","title":"Home File System","text":"<p>After logging in, users are placed in their home directory, /home, also referenced by the environment variable <code>$HOME</code>. The home directory is limited in space and should be used only for source code storage. User will have access to 100GB in <code>/home</code>. </p> <p>Jobs should never be run from the home file system, as it is not set up for high performance throughput. Users should keep usage on <code>$HOME</code> under 100GB. </p> <p>Backups are currently being stored on a rolling 8-week period. In case of file corruption/data loss, please contact us at ACCESS Help Desk to retrieve the requested files.</p>"},{"location":"system_access/","title":"System Access","text":"<p>As an ACCESS computing resource, Expanse is accessible to ACCESS users who are given time on the system. To obtain an account, users may submit a proposal through the ACCESS Allocation Request System  or request a Trial Account. Interested parties may contact the ACCESS Help Desk for help with an Expanse proposal (see sidebar for contact information).</p>"},{"location":"system_access/#logging-in-to-expanse","title":"Logging in to Expanse","text":"<p>Expanse supports access via the command line using an ACCESS-wide password or ssh-keys with TOTP (see setup instructions below), and web-based access via the  Expanse User Portal. While CPU and GPU resources are allocated separately, the login nodes are the same. To log in to Expanse from the command line, use the hostname:</p> <pre><code>login.expanse.sdsc.edu\n</code></pre> <p>The following are examples of Secure Shell (ssh) commands that may be used to log in to Expanse:</p> <pre><code>ssh &lt;your_username&gt;@login.expanse.sdsc.edu\nssh -l &lt;your_username&gt; login.expanse.sdsc.edu\n</code></pre>"},{"location":"system_access/#notes-and-hints","title":"Notes and hints","text":"<ul> <li>When you log in to expanse.sdsc.edu, you will be assigned one of the two login nodes login0[1-2]-expanse.sdsc.edu. These nodes are identical in both architecture and software environment. Users should normally log in through login.expanse.sdsc.edu, but may specify one of the two nodes directly if they see poor performance.</li> <li>Please feel free to append your public key to your ~/.ssh/authorized_keys file to enable access from authorized hosts without having to enter your password. We accept RSA, ECDSA and ed25519 keys. Make sure you have a strong passphrase on the private key on your local machine.<ul> <li>You can use ssh-agent or keychain to avoid repeatedly typing the private key password.</li> <li>Hosts which connect to SSH more frequently than ten times per minute may get blocked for a short period of time</li> </ul> </li> <li>Do not use the login nodes for computationally intensive processes, as hosts for running workflow management tools, as primary data transfer nodes for large or numerous data transfers or as servers providing other services accessible to the Internet. The login nodes are meant for file editing, simple data analysis, and other tasks that use minimal compute resources. All computationally demanding jobs should be submitted and run through the batch queuing system.</li> <li>Login nodes are not the same as the batch nodes, Users should request an interactive sessions to compile programs.</li> </ul>"},{"location":"system_access/#2fa-with-authenticator-authenticator-required","title":"2FA with Authenticator Authenticator (Required)","text":"<p>Expanse uses two-factor authentication (2FA) when using a password to log in. 2FA adds a layer of security to your authentication process.</p>"},{"location":"system_access/#install-authenticator-app","title":"Install Authenticator App:","text":"<p>Users will first need to Install an authenticator app on their smartphone or other device. Users can use any app that supports importing TOTP 2FA codes with a QR code. (Google Authenticator, DUO Mobile App, LastPass Authenticator App, etc) We suggest using the Google Authenticator app if you do not an athenticator application already istalled on your mobile device.</p> <ul> <li>Google Authenticator for Apple IOS</li> <li>Google Authenticator for Android</li> </ul> <p>Once the authenticator app has been installed, users will need to enroll and pair the 2FA device with their Expanse Account.</p>"},{"location":"system_access/#to-enroll","title":"To enroll:","text":"<ol> <li>Go to the website https://passive.sdsc.edu</li> <li>Select the appropriate option:<ul> <li>Login using Globus \u2013 Select if you are an ACCESS allocated user. You will be redirected to the Globus OpenID connect authorization site. Please choose ACCESS CI as the organization (not your home institution).</li> <li>I have an invitation \u2013 Select if you are NOT an ACCESS allocated user. You will need to contact SDSC support at consult@sdsc.edu to receive an invitation.</li> </ul> </li> <li> <p>Once logged in you can manage your 2FA enrollment. Click on the \u201cManage 2FA\u201d button and follow the instructions to scan the QR code. Once the pairing is complete you can close the browser. (NOTE: This change make take up to 15 minutes to update on the system)</p> <p><code>bash ssh &lt;your_username&gt;@login.expanse.sdsc.edu ssh -l &lt;your_username&gt; login.expanse.sdsc.edu</code></p> </li> </ol> <p>You will either be prompted for a password (for ACCESS users this is their ACCESS portal password) followed by a prompt for the TOTP number from your authenticator app OR if you have a ssh key in place on Expanse, and the corresponding private key loaded in your ssh-agent, you will skip straight to the prompt for your TOTP code.</p> <p>Note: If you already enrolled in 2FA before Feb 24, 2025, you do not need to re-enroll as we will incorporate your existing 2FA information into the new setup.</p>"},{"location":"system_access/#expanse-user-portal","title":"Expanse User Portal","text":"<p>The Expanse User Portal provides a quick and easy way for Expanse users to log in, transfer and edit files, and submit and monitor jobs. The Portal provides a gateway for launching interactive applications such as MATLAB, RStudio, and an integrated web-based environment for file management and job submission. All ACCESS users with a valid Expanse allocation have access via their ACCESS-based credentials.</p>"},{"location":"technical_summary/","title":"Technical Summary","text":"<p>Expanse is a dedicated Advanced Cyberinfrastructure Coordination Ecosystem: Services and Support (ACCESS) cluster designed by Dell and SDSC delivering 5.16 peak petaflops, and will offer Composable Systems and Cloud Bursting. Expanse's standard compute nodes are each powered by two 64-core AMD EPYC 7742 processors and contain 256 GB of DDR4 memory, while each GPU node contains four NVIDIA V100s (32 GB SXM2) connected via NVLINK and dual 20-core Intel Xeon 6248 CPUs. Expanse also has four 2 TB large memory nodes. Expanse is organized into 13 SDSC Scalable Compute Units (SSCUs), comprising 728 standard nodes, 54 GPU nodes and 4 large-memory nodes. Every Expanse node has access to a 12 PB Lustre parallel file system (provided by Aeon Computing) and a 7 PB Ceph Object Store system. Expanse uses the Bright Computing HPC Cluster management system and the SLURM workload manager for job scheduling. Expanse supports the ACCESS core software stack, which includes remote login, remote computation, data movement, science workflow support, and science gateway support toolkits. Expanse is an NSF-funded system operated by the San Diego Supercomputer Center at UC San Diego, and is available through the ACCESS program.</p>"},{"location":"technical_summary/#resource-allocation-policies","title":"Resource Allocation Policies","text":"<ul> <li>The maximum allocation for a Principle Investigator on Expanse is 15M core-hours and 75K GPU hours. Limiting the allocation size means that Expanse can support more projects, since the average size of each is smaller.</li> <li>Science Gateways requesting in the Maximize tier can request up to 30M core-hours.</li> </ul>"},{"location":"technical_summary/#job-scheduling-policies","title":"Job Scheduling Policies","text":"<ul> <li>The maximum allowable job size on Expanse is 4,096 cores \u2013 a limit that helps shorten wait times since there are fewer nodes in idle state waiting for large number of nodes to become free.</li> <li>Expanse supports long-running jobs - run times can be extended to one week. Users requests will be evaluated based on number of jobs and job size.</li> <li>Expanse supports shared-node jobs (more than one job on a single node). Many applications are serial or can only scale to a few cores. Allowing shared nodes improves job throughput, provides higher overall system utilization, and allows more users to run on Expanse.</li> </ul>"},{"location":"technical_summary/#technical-details","title":"Technical Details","text":"System Component Configuration Compute Nodes CPU Type AMD EPYC 7742 Nodes 728 Sockets 2 Cores/socket 64 Clock speed 2.25 GHz Flop speed 4608 GFlop/s Memory capacity 256 GB DDR4 DRAM Local Storage 1TB Intel P4510 NVMe PCIe SSD Max CPU Memory bandwidth 409.5 GB/s GPU Nodes GPU Type NVIDIA V100 SXM2 Nodes 52 GPUs/node 4 CPU Type Xeon Gold 6248 Cores/socket 20 Sockets 2 Clock speed 2.5 GHz Flop speed 34.4 TFlop/s Memory capacity 384 GB DDR4 DRAM Local Storage 1.6TB Samsung PM1745b NVMe PCIe SSD Max CPU Memory bandwidth 281.6 GB/s Large-Memory CPU Type AMD EPYC 7742 Nodes 4 Sockets 2 Cores/socket 64 Clock speed 2.25 GHz Flop speed 4608 GFlop/s Memory capacity 2 TB Local Storage 3.2 TB (2 X 1.6 TB Samsung PM1745b NVMe PCIe SSD) STREAM Triad bandwidth ~310 GB/sec Full System Total compute nodes 728 Total compute cores 93,184 Total GPU nodes 52 Total V100 GPUs 208 Peak performance 5.16 PFlop/s Total memory 247 TB Total memory bandwidth 215 TB/s Total flash memory 824 TB HDR InfiniBand Interconnect Topology Hybrid Fat-Tree Link bandwidth 100 Gb/s (bidirectional); HDR200 switches Peak bisection bandwidth 8.5 TB/s MPI latency 1.17-1.69 \u00b5s DISK I/O Subsystem File Systems NFS, Ceph Lustre Storage(performance) 12 PB Ceph Storage 7 PB I/O bandwidth (performance disk) 140 GB/s, 200K IOPs"},{"location":"technical_summary/#systems-software-environment","title":"Systems Software Environment","text":"Software Function Description Cluster Management Bright Cluster Manager Operating System Rocky Linux File Systems Lustre, Ceph Scheduler and Resource Manager SLURM ACCESS Software CTSS User Environment Lmod Compilers AOCC, GCC, Intel, PGI Message Passing Intel MPI, MVAPICH, Open MPI"},{"location":"using_gpu_nodes/","title":"Using GPU Nodes","text":"<p>GPU nodes are allocated as a separate resource. The GPU nodes can be accessed via either the \"gpu\" or the \"gpu-shared\" partitions.</p> <pre><code>#SBATCH -p gpu\n</code></pre> <p>or</p> <pre><code>#SBATCH -p gpu-shared\n</code></pre> <p>When users request 1 GPU, in gpu-shared partition, by default they will also receive, 1 CPU, and 1G memory. Here is an example AMBER script using the gpu-shared queue.</p>"},{"location":"using_gpu_nodes/#gpu-job","title":"GPU job","text":"<pre><code>#!/bin/bash\n#SBATCH --job-name=\"ambergpu\"\n#SBATCH --output=\"ambergpu.%j.%N.out\"\n#SBATCH --partition=gpu\n#SBATCH --nodes=1\n#SBATCH --gpus=4\n#SBATCH --mem=377300M\n#SBATCH --account=&lt;&lt;project*&gt;&gt;\n#SBATCH --no-requeue\n#SBATCH -t 01:00:00\n\nmodule purge\nmodule load gpu\nmodule load slurm\nmodule load openmpi\nmodule load amber\npmemd.cuda -O -i mdin.GPU -o mdout.GPU.$SLURM_JOBID -x mdcrd.$SLURM_JOBID \\\n    -nf mdinfo.$SLURM_JOBID -1 mdlog.$SLURM_JOBID -p prmtop -c inpcrd\n</code></pre> <p>* Expanse requires users to enter a valid project name; users can list valid project by running the<code>expanse-client</code> script.</p>"},{"location":"using_gpu_nodes/#gpu-shared-job","title":"GPU-shared job","text":"<pre><code>#!/bin/bash\n#SBATCH --job-name=\"ambergpushared\"\n#SBATCH --output=\"ambergpu.%j.%N.out\"\n#SBATCH --partition=gpu-shared\n#SBATCH --nodes=1\n#SBATCH --gpus=2\n#SBATCH --cpus-per-task=1\n#SBATCH --mem=93G\n#SBATCH --account=&lt;&lt;project*&gt;&gt;\n#SBATCH --no-requeue\n#SBATCH -t 01:00:00\n\nmodule purge\nmodule load gpu\nmodule load slurm\nmodule load openmpi\nmodule load amber\npmemd.cuda -O -i mdin.GPU -o mdout-OneGPU.$SLURM_JOBID -p prmtop -c inpcrd\n</code></pre> <p>* Expanse requires users to enter a valid project name; users can list valid project by running the<code>expanse-client</code> script.</p> <p>Users can find application specific example job script on the system in directory <code>/cm/shared/examples/sdsc/</code>. GPU modes can be controlled for jobs in the \"gpu\" partition. By default, the GPUs are in non-exclusive mode and the persistence mode is 'on'. If a particular \"gpu\" partition job needs exclusive access the following options should be set in your batch script:</p> <pre><code>#SBATCH --constraint=exclusive\n</code></pre> <p>To turn persistence off add the following line to your batch script:</p> <pre><code>#SBATCH --constraint=persistenceoff\n</code></pre> <p>The charging equation will be:</p> <p>GPU SUs = (Number of GPUs) x (wallclock time)</p>"},{"location":"using_gpu_nodes/#slurm-no-requeue-option","title":"SLURM No-Requeue Option","text":"<p>SLURM will requeue jobs if there is a node failure. However, in some cases this might be detrimental if files get overwritten. If users wish to avoid automatic requeue, the following line should be added to their script:</p> <pre><code>#SBATCH --no-requeue\n</code></pre> <p>The 'requeue' count limit is currently set to 5. The job will be requeued 5 times after which the job will be placed in the REQUEUE_HOLD state and the job must be canceled and resubmitted.</p>"},{"location":"using_gpu_nodes/#example-scripts-for-applications","title":"Example Scripts for Applications","text":"<p>SDSC User Services staff have developed sample run scripts for common applications. They are available in the <code>/cm/shared/examples</code> directory on Expanse.</p>"},{"location":"using_gpu_nodes/#job-dependencies","title":"Job Dependencies","text":"<p>There are several scenarios (e.g. splitting long running jobs, workflows) where users may require jobs with dependencies on successful completions of other jobs. In such cases, SLURM's --dependency option can be used. The syntax is as follows:</p> <pre><code>[user@login01-expanse ~]$ sbatch --dependency=afterok:jobid jobscriptfile\n</code></pre>"},{"location":"using_gpu_nodes/#job-monitoring-and-management","title":"Job Monitoring and Management","text":"<p>Users can monitor jobs using the squeue command.</p> <pre><code>[user@expanse ~]$ squeue -u user1\n             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n            256556   compute raxml_na user1     R    2:03:57      4 expanse-14-[11-14]\n            256555   compute raxml_na user1     R    2:14:44      4 expanse-02-[06-09]\n</code></pre> <p>In this example, the output lists two jobs that are running in the \"compute\" partition. The jobID, partition name, job names, user names, status, time, number of nodes, and the node list are provided for each job. Some common squeue options include:</p> Option Result -i  Repeatedly report at intervals (in seconds) -ij Displays information for specified job(s) -p  Displays information for specified partitions (queues) -t  Shows jobs in the specified state(s) <p>Users can cancel their own jobs using the scancel command as follows:</p> <pre><code>[user@expanse ~]$ scancel &lt;jobid&gt;\n</code></pre>"}]}